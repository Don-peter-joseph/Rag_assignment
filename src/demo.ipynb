{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f4f6e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "## Reducers\n",
    "from typing import Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import AnyMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e91385a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages:Annotated[list[AnyMessage],add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4846daac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING\"]=os.getenv(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]= os.getenv(\"LANGSMITH_ENDPOINT\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGSMITH_TRACING\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGSMITH_PROJECT\")\n",
    "os.environ[\"GROQ_API_KEY\"] =os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"OPENWEATHERMAP_API_KEY\"]=os.getenv(\"OPENWEATHERMAP_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9cd2dc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x1101bd190>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 4fb392fc-e80a-4ca4-92db-d5b2d5c54599)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x1101bc4a0>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 032c7208-819f-4adf-ba20-23142bd44a79)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161b6ab10>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 49a7d62c-2d9f-495a-95c9-12f215d4d122)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161b6bce0>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 761aaccc-91b7-4d39-9127-4b531688db6e)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161b68650>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 8b86f6a3-761a-4b06-8f8b-f895b687310e)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161b69550>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: fcdea072-2398-45fc-8852-6e5c85ce12b0)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/config.json\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x1101bd010>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: e065151b-9b13-4fa3-8ecf-ca6e7f8caf49)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x1101bd070>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 2b156210-9fc2-4016-8dfd-9208a624fdf1)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x116387a70>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: fdfb1edb-a74c-4731-b927-b9f836d82037)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161b6b410>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 22faf159-cbd1-4c9f-bf2c-189598ca319d)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161b69eb0>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: e5a618ba-8e12-4dd5-a863-035f37484aa1)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161b6bc50>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 96df0537-34d9-475b-afa5-a0f2bb0f5b8d)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/config.json\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/preprocessor_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161b73140>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: b3a1a221-41f8-494a-be4d-26557e16dd39)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/preprocessor_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/preprocessor_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161b451f0>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 1b1ba262-5387-4dd3-915c-449c05c2fa09)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/preprocessor_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/preprocessor_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161bb6570>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 473cc725-1d62-408e-87dc-f53b1d981f2c)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/preprocessor_config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/preprocessor_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161bc3cb0>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 5b9751bd-4e9e-4863-8200-b3128e864c62)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/preprocessor_config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/preprocessor_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161bc2720>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 7ee55e2d-88c9-4b2a-9d51-3be967c47424)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/preprocessor_config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/preprocessor_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161bc37a0>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 8b3302fd-a9cf-4c33-a662-36df6bea3dc7)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/preprocessor_config.json\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/processor_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161b6df70>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 9411f025-97f5-44c4-8923-750805fe0c6e)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/processor_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/processor_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161bbc7a0>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 04554cee-8f7e-4579-a882-08c89e28ef83)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/processor_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/processor_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161bbeae0>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 2deb37b0-6ff6-471f-9a2c-c0f0c4489b93)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/processor_config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/processor_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161bbf830>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 16d01cb5-a95f-448c-abcb-91021fd65b5f)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/processor_config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/processor_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161bc5c40>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 681bd3f7-a7d8-47c5-85c2-d107c8c65fe9)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/processor_config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/processor_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161bc4140>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: fe530808-a3e7-43ae-aec5-5ce58a1cad85)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/processor_config.json\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161bc2570>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 0c723c71-91db-456e-848e-a968a8646ff9)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161bc3680>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: d6ee1a31-c4bf-429b-941c-cc2f4770405d)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/tokenizer_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161bc2f00>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: b611fcff-c88f-4ec7-9ff8-5d70eaf81561)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/tokenizer_config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x1617f9610>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: a561d081-ae92-4754-aeec-1a7ca3260610)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161b72cc0>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: ea6b215d-defc-4f43-9094-5eb34b474518)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161b6b890>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 0bb0ede8-efc7-44d9-8c24-7a1f9a69c5c0)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/tokenizer_config.json\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/processor_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161b68620>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 08871aea-d83d-40d9-996c-37203efb74d6)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/processor_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/processor_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161bc3890>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: b1787718-dd20-41f2-8686-cf026065f2de)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/processor_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/processor_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161bc26c0>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: cfedb5f6-de53-457b-bb6c-289cbeb0172f)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/processor_config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/processor_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161bc31a0>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 968170e9-269f-4792-b619-903879c9317c)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/processor_config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/processor_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x161b6ec60>: Failed to resolve \\'huggingface.co\\' ([Errno 8] nodename nor servname provided, or not known)\"))'), '(Request ID: 7821d371-6d9e-4862-96c2-59e8849a9e83)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/processor_config.json\n",
      "Retrying in 8s [Retry 5/5].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import CLIPProcessor,CLIPModel\n",
    "from PIL import Image\n",
    "import base64\n",
    "\n",
    "clip_model=CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n",
    "clip_processor=CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "clip_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "31bdb216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding function for images\n",
    "import torch\n",
    "\n",
    "def embed_image(image_data):\n",
    "    if isinstance(image_data,str): #path\n",
    "        image=Image.open(image_data).convert('RGB')\n",
    "    else:\n",
    "        image=image_data\n",
    "\n",
    "    inputs=clip_processor(images=image,return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        features=clip_model.get_image_features(**inputs)\n",
    "        #normalize to unit vector\n",
    "        features=features/features.norm(dim=-1,keepdim=True)\n",
    "        return features.squeeze().numpy()\n",
    "    \n",
    "def embed_text(text):\n",
    "    inputs=clip_processor(\n",
    "        text=text,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=77\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        features=clip_model.get_text_features(**inputs)\n",
    "        #normalize to unit vector\n",
    "        features=features/features.norm(dim=-1,keepdim=True)\n",
    "        return features.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc2b3707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process pdf\n",
    "import fitz #pymupdf\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages import HumanMessage,AIMessage\n",
    "from langchain_core.prompts import ChatMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "pdf_path='../files/gemma.pdf'\n",
    "doc=fitz.open(pdf_path)\n",
    "\n",
    "\n",
    "all_docs=[]\n",
    "all_embeddings=[]\n",
    "image_data_store={}\n",
    "\n",
    "splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a330a6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "import io\n",
    "\n",
    "for i,page in enumerate(doc):\n",
    "    #process text \n",
    "    text=page.get_text()\n",
    "    if text.strip():\n",
    "        temp_doc=Document(page_content=text,metadata={'page':i,'type':'text'})\n",
    "        text_chunks=splitter.split_documents([temp_doc])\n",
    "        for chunk in text_chunks:\n",
    "            embedding=embed_text(chunk.page_content)\n",
    "            all_embeddings.append(embedding)\n",
    "            all_docs.append(chunk)\n",
    "\n",
    "    #process images\n",
    "    #convert pdf images to pil\n",
    "    #convert to base64\n",
    "    #clip embedding for retrieval\n",
    "    for img_id,img in enumerate(page.get_images(full=True)):\n",
    "        try:\n",
    "            xref=img[0]\n",
    "            base_image=doc.extract_image(xref)\n",
    "            image_bytes=base_image['image']\n",
    "\n",
    "            #convert to pil\n",
    "            pil_image=Image.open(io.BytesIO(image_bytes)).convert('RGB')\n",
    "\n",
    "            #image identifier\n",
    "            image_id=f'page_{i}_img_{img_id}'\n",
    "\n",
    "            buffered=io.BytesIO()\n",
    "            pil_image.save(buffered,format=\"PNG\")\n",
    "            image_base64=base64.b64encode(buffered.getvalue()).decode()\n",
    "            image_data_store[image_id]=image_base64\n",
    "\n",
    "            #embed using clip\n",
    "            embedding=embed_image(pil_image)\n",
    "            all_embeddings.append(embedding)\n",
    "\n",
    "            #document for image\n",
    "            image_doc=Document(\n",
    "                page_content=f'[image:{image_id}]',\n",
    "                metadata={'page':i,'type':'image','image_id':image_id}\n",
    "            )\n",
    "            all_docs.append(image_doc)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e,img_id)\n",
    "            continue\n",
    "doc.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c80bd168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 0, 'type': 'text'}, page_content='3/10/24, 8:32 PM\\nGemma: Google introduces new state-of-the-art open models\\nhttps://blog.google/technology/developers/gemma-open-models/\\n1/5\\nDEVELOPERS\\nGemma: Introducing new state-\\nof-the-art open models\\nGemma is built for responsible AI development from the same research and\\ntechnology used to create Gemini models.\\nFeb 21, 2024 · 3 min read\\nJ\\nJeanine Banks\\nVP & GM, Developer X and\\nDevRel\\nT\\nTris Warkentin\\nDirector, Google DeepMind\\nShare\\nListen to article 7 minutes\\nThe Keyword'),\n",
       " Document(metadata={'page': 0, 'type': 'image', 'image_id': 'page_0_img_0'}, page_content='[image:page_0_img_0]'),\n",
       " Document(metadata={'page': 1, 'type': 'text'}, page_content='3/10/24, 8:32 PM\\nGemma: Google introduces new state-of-the-art open models\\nhttps://blog.google/technology/developers/gemma-open-models/\\n2/5\\nAt Google, we believe in making AI helpful for everyone. We have a long history of contributing innovations\\nto the open community, such as with Transformers, TensorFlow, BERT, T5, JAX, AlphaFold, and AlphaCode.\\nToday, we’re excited to introduce a new generation of open models from Google to assist developers and\\nresearchers in building AI responsibly.'),\n",
       " Document(metadata={'page': 1, 'type': 'text'}, page_content='researchers in building AI responsibly.\\nGemma open models\\nGemma is a family of lightweight, state-of-the-art open models built from the same research and\\ntechnology used to create the Gemini models. Developed by Google DeepMind and other teams across\\nGoogle, Gemma is inspired by Gemini, and the name reflects the Latin gemma, meaning “precious stone.”\\nAccompanying our model weights, we’re also releasing tools to support developer innovation, foster'),\n",
       " Document(metadata={'page': 1, 'type': 'text'}, page_content='collaboration, and guide responsible use of Gemma models.\\nGemma is available worldwide, starting today. Here are the key details to know:\\nWe’re releasing model weights in two sizes: Gemma 2B and Gemma 7B. Each size is released with pre-\\ntrained and instruction-tuned variants.\\nA new Responsible Generative AI Toolkit provides guidance and essential tools for creating safer AI\\napplications with Gemma.'),\n",
       " Document(metadata={'page': 1, 'type': 'text'}, page_content='applications with Gemma.\\nWe’re providing toolchains for inference and supervised fine-tuning (SFT) across all major frameworks:\\nJAX, PyTorch, and TensorFlow through native Keras 3.0.\\nReady-to-use Colab and Kaggle notebooks, alongside integration with popular tools such as Hugging\\nFace, MaxText, NVIDIA NeMo and TensorRT-LLM, make it easy to get started with Gemma.\\nPre-trained and instruction-tuned Gemma models can run on your laptop, workstation, or Google Cloud'),\n",
       " Document(metadata={'page': 1, 'type': 'text'}, page_content='with easy deployment on Vertex AI and Google Kubernetes Engine (GKE).\\nOptimization across multiple AI hardware platforms ensures industry-leading performance, including\\nNVIDIA GPUs and Google Cloud TPUs.\\nTerms of use permit responsible commercial usage and distribution for all organizations, regardless of\\nsize.\\nState-of-the-art performance at size\\nGemma models share technical and infrastructure components with Gemini, our largest and most capable'),\n",
       " Document(metadata={'page': 1, 'type': 'text'}, page_content='AI model widely available today. This enables Gemma 2B and 7B to achieve best-in-class performance for'),\n",
       " Document(metadata={'page': 2, 'type': 'text'}, page_content='3/10/24, 8:32 PM\\nGemma: Google introduces new state-of-the-art open models\\nhttps://blog.google/technology/developers/gemma-open-models/\\n3/5\\ntheir sizes compared to other open models. And Gemma models are capable of running directly on a\\ndeveloper laptop or desktop computer. Notably, Gemma surpasses significantly larger models on key\\nbenchmarks while adhering to our rigorous standards for safe and responsible outputs. See the technical'),\n",
       " Document(metadata={'page': 2, 'type': 'text'}, page_content='report for details on performance, dataset composition, and modeling methodologies.\\nResponsible by design\\nGemma is designed with our AI Principles at the forefront. As part of making Gemma pre-trained models\\nsafe and reliable, we used automated techniques to filter out certain personal information and other\\nsensitive data from training sets. Additionally, we used extensive fine-tuning and reinforcement learning'),\n",
       " Document(metadata={'page': 2, 'type': 'text'}, page_content='from human feedback (RLHF) to align our instruction-tuned models with responsible behaviors. To\\nunderstand and reduce the risk profile for Gemma models, we conducted robust evaluations including\\nmanual red-teaming, automated adversarial testing, and assessments of model capabilities for dangerous\\nactivities. These evaluations are outlined in our Model Card.\\nWe’re also releasing a new Responsible Generative AI Toolkit together with Gemma to help developers and'),\n",
       " Document(metadata={'page': 2, 'type': 'text'}, page_content='researchers prioritize building safe and responsible AI applications. The toolkit includes:\\n1'),\n",
       " Document(metadata={'page': 2, 'type': 'image', 'image_id': 'page_2_img_0'}, page_content='[image:page_2_img_0]'),\n",
       " Document(metadata={'page': 3, 'type': 'text'}, page_content=\"3/10/24, 8:32 PM\\nGemma: Google introduces new state-of-the-art open models\\nhttps://blog.google/technology/developers/gemma-open-models/\\n4/5\\nSafety classification: We provide a novel methodology for building robust safety classifiers with\\nminimal examples.\\nDebugging: A model debugging tool helps you investigate Gemma's behavior and address potential\\nissues.\\nGuidance: You can access best practices for model builders based on Google’s experience in\\ndeveloping and deploying large language models.\"),\n",
       " Document(metadata={'page': 3, 'type': 'text'}, page_content='developing and deploying large language models.\\nOptimized across frameworks, tools and hardware\\nYou can fine-tune Gemma models on your own data to adapt to specific application needs, such as\\nsummarization or retrieval-augmented generation (RAG). Gemma supports a wide variety of tools and\\nsystems:\\nMulti-framework tools: Bring your favorite framework, with reference implementations for inference\\nand fine-tuning across multi-framework Keras 3.0, native PyTorch, JAX, and Hugging Face\\nTransformers.'),\n",
       " Document(metadata={'page': 3, 'type': 'text'}, page_content='Transformers.\\nCross-device compatibility: Gemma models run across popular device types, including laptop,\\ndesktop, IoT, mobile and cloud, enabling broadly accessible AI capabilities.\\nCutting-edge hardware platforms: We’ve partnered with NVIDIA to optimize Gemma for NVIDIA GPUs,\\nfrom data center to the cloud to local RTX AI PCs, ensuring industry-leading performance and\\nintegration with cutting-edge technology.'),\n",
       " Document(metadata={'page': 3, 'type': 'text'}, page_content='integration with cutting-edge technology.\\nOptimized for Google Cloud: Vertex AI provides a broad MLOps toolset with a range of tuning options\\nand one-click deployment using built-in inference optimizations. Advanced customization is available\\nwith fully-managed Vertex AI tools or with self-managed GKE, including deployment to cost-efficient\\ninfrastructure across GPU, TPU, and CPU from either platform.\\nFree credits for research and development'),\n",
       " Document(metadata={'page': 3, 'type': 'text'}, page_content='Free credits for research and development\\nGemma is built for the open community of developers and researchers powering AI innovation. You can\\nstart working with Gemma today using free access in Kaggle, a free tier for Colab notebooks, and $300 in\\ncredits for first-time Google Cloud users. Researchers can also apply for Google Cloud credits of up to a\\ncollective $500,000 to accelerate their projects.'),\n",
       " Document(metadata={'page': 4, 'type': 'text'}, page_content='3/10/24, 8:32 PM\\nGemma: Google introduces new state-of-the-art open models\\nhttps://blog.google/technology/developers/gemma-open-models/\\n5/5\\nGetting started\\nYou can explore more about Gemma and access quickstart guides on ai.google.dev/gemma.\\nAs we continue to expand the Gemma model family, we look forward to introducing new variants for\\ndiverse applications. Stay tuned for events and opportunities in the coming weeks to connect, learn and\\nbuild with Gemma.\\nWe’re excited to see what you create!'),\n",
       " Document(metadata={'page': 4, 'type': 'text'}, page_content='We’re excited to see what you create!\\nPOSTED IN:\\nDevelopers\\n \\nAI\\nMore Information\\nCollapse\\nGoogle adheres to rigorous data filtering practices to ensure fair evaluation. Our models exclude\\nbenchmark data from training sets, ensuring the integrity of benchmark comparisons.\\n1')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ed9b84f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "sparse_retriever=BM25Retriever.from_documents(all_docs)\n",
    "sparse_retriever.k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1765b870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0p/n1mxyw8j7s56r6nh6qdnfnf40000gn/T/ipykernel_1214/4040595156.py:27: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance, PointStruct\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_core.embeddings import Embeddings\n",
    "import uuid\n",
    "import numpy as np\n",
    "\n",
    "class CLIPEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts):\n",
    "        return [embed_text(t) for t in texts]\n",
    "    def embed_query(self, text):\n",
    "        return embed_text(text)\n",
    "\n",
    "clip_embeddings = CLIPEmbeddings()\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ Create in-memory Qdrant\n",
    "# -----------------------------\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "collection_name = \"report\"\n",
    "\n",
    "# Infer embedding size\n",
    "vector_dim = len(all_embeddings[0])\n",
    "\n",
    "# Create collection manually\n",
    "client.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=vector_dim, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Upload existing embeddings\n",
    "# -----------------------------\n",
    "points = [\n",
    "    PointStruct(\n",
    "        id=str(uuid.uuid4()),\n",
    "        vector=emb.tolist() if isinstance(emb, np.ndarray) else emb,\n",
    "        payload={\n",
    "            \"page_content\": doc.page_content,\n",
    "            \"metadata\": doc.metadata,\n",
    "        },\n",
    "    )\n",
    "    for doc, emb in zip(all_docs, all_embeddings)\n",
    "]\n",
    "\n",
    "client.upsert(collection_name=collection_name, points=points)\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Wrap with LangChain store\n",
    "# -----------------------------\n",
    "vectorstore = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embedding=clip_embeddings,  # since embeddings are precomputed\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6d495812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 1, 'type': 'text'} → applications with Gemma.\n",
      "We’re providing toolchains for inference and supervised fine-tuning (SFT) a\n",
      "-----------\n",
      "{'page': 4, 'type': 'text'} → We’re excited to see what you create!\n",
      "POSTED IN:\n",
      "Developers\n",
      " \n",
      "AI\n",
      "More Information\n",
      "Collapse\n",
      "Google ad\n",
      "-----------\n",
      "{'page': 1, 'type': 'text'} → AI model widely available today. This enables Gemma 2B and 7B to achieve best-in-class performance f\n",
      "-----------\n",
      "{'page': 2, 'type': 'text'} → researchers prioritize building safe and responsible AI applications. The toolkit includes:\n",
      "1\n",
      "-----------\n",
      "{'page': 1, 'type': 'text'} → with easy deployment on Vertex AI and Google Kubernetes Engine (GKE).\n",
      "Optimization across multiple A\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0p/n1mxyw8j7s56r6nh6qdnfnf40000gn/T/ipykernel_1214/3458981884.py:4: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n"
     ]
    }
   ],
   "source": [
    "query = \"which are some multiframework tools\"\n",
    "query_vector = embed_text(query)\n",
    "\n",
    "results = client.search(\n",
    "    collection_name=collection_name,\n",
    "    query_vector=query_vector,\n",
    "    with_payload=True,\n",
    "    limit=5,\n",
    ")\n",
    "\n",
    "for r in results:\n",
    "    print(r.payload[\"metadata\"], \"→\", r.payload[\"page_content\"][:100])\n",
    "    print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8d65e941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm=ChatGroq(model=\"openai/gpt-oss-120b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1d6d8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstore.as_retriever(search_kwargs={'k':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81b7ba78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0p/n1mxyw8j7s56r6nh6qdnfnf40000gn/T/ipykernel_1214/2899309716.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retriever.get_relevant_documents('what is gemma')\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 1, 'type': 'text', '_id': '648549cd-80b2-4388-bc5a-6b188a6c9dff', '_collection_name': 'report'}, page_content='AI model widely available today. This enables Gemma 2B and 7B to achieve best-in-class performance for'),\n",
       " Document(metadata={'page': 2, 'type': 'text', '_id': '572e322f-f886-494a-9079-6fc165281826', '_collection_name': 'report'}, page_content='3/10/24, 8:32 PM\\nGemma: Google introduces new state-of-the-art open models\\nhttps://blog.google/technology/developers/gemma-open-models/\\n3/5\\ntheir sizes compared to other open models. And Gemma models are capable of running directly on a\\ndeveloper laptop or desktop computer. Notably, Gemma surpasses significantly larger models on key\\nbenchmarks while adhering to our rigorous standards for safe and responsible outputs. See the technical'),\n",
       " Document(metadata={'page': 1, 'type': 'text', '_id': '6ace67ea-ae3a-4915-a1b7-c69319bcf16c', '_collection_name': 'report'}, page_content='3/10/24, 8:32 PM\\nGemma: Google introduces new state-of-the-art open models\\nhttps://blog.google/technology/developers/gemma-open-models/\\n2/5\\nAt Google, we believe in making AI helpful for everyone. We have a long history of contributing innovations\\nto the open community, such as with Transformers, TensorFlow, BERT, T5, JAX, AlphaFold, and AlphaCode.\\nToday, we’re excited to introduce a new generation of open models from Google to assist developers and\\nresearchers in building AI responsibly.'),\n",
       " Document(metadata={'page': 1, 'type': 'text', '_id': 'fe410874-c1de-4858-a87e-23402cfbbeb3', '_collection_name': 'report'}, page_content='applications with Gemma.\\nWe’re providing toolchains for inference and supervised fine-tuning (SFT) across all major frameworks:\\nJAX, PyTorch, and TensorFlow through native Keras 3.0.\\nReady-to-use Colab and Kaggle notebooks, alongside integration with popular tools such as Hugging\\nFace, MaxText, NVIDIA NeMo and TensorRT-LLM, make it easy to get started with Gemma.\\nPre-trained and instruction-tuned Gemma models can run on your laptop, workstation, or Google Cloud'),\n",
       " Document(metadata={'page': 1, 'type': 'text', '_id': 'a081a67a-615b-452d-98ef-4009ceff5626', '_collection_name': 'report'}, page_content='collaboration, and guide responsible use of Gemma models.\\nGemma is available worldwide, starting today. Here are the key details to know:\\nWe’re releasing model weights in two sizes: Gemma 2B and Gemma 7B. Each size is released with pre-\\ntrained and instruction-tuned variants.\\nA new Responsible Generative AI Toolkit provides guidance and essential tools for creating safer AI\\napplications with Gemma.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents('what is gemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "019faadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "hybrid_retriever=EnsembleRetriever(\n",
    "    retrievers=[retriever,sparse_retriever],\n",
    "    weights=[0.6,0.4]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b7822a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 4, 'type': 'text', '_id': 'ec8d5a13-2881-49c0-b90b-321780ab7525', '_collection_name': 'report'}, page_content='We’re excited to see what you create!\\nPOSTED IN:\\nDevelopers\\n \\nAI\\nMore Information\\nCollapse\\nGoogle adheres to rigorous data filtering practices to ensure fair evaluation. Our models exclude\\nbenchmark data from training sets, ensuring the integrity of benchmark comparisons.\\n1'),\n",
       " Document(metadata={'page': 1, 'type': 'text', '_id': 'e1df1d04-efc1-48a9-818e-2bfe8480c155', '_collection_name': 'report'}, page_content='AI model widely available today. This enables Gemma 2B and 7B to achieve best-in-class performance for'),\n",
       " Document(metadata={'page': 2, 'type': 'text', '_id': 'f14e6203-6e1e-421f-ac74-546490480d30', '_collection_name': 'report'}, page_content='researchers prioritize building safe and responsible AI applications. The toolkit includes:\\n1'),\n",
       " Document(metadata={'page': 1, 'type': 'text', '_id': '8ade314c-b21f-4f7a-86c9-25f95c7c214c', '_collection_name': 'report'}, page_content='applications with Gemma.\\nWe’re providing toolchains for inference and supervised fine-tuning (SFT) across all major frameworks:\\nJAX, PyTorch, and TensorFlow through native Keras 3.0.\\nReady-to-use Colab and Kaggle notebooks, alongside integration with popular tools such as Hugging\\nFace, MaxText, NVIDIA NeMo and TensorRT-LLM, make it easy to get started with Gemma.\\nPre-trained and instruction-tuned Gemma models can run on your laptop, workstation, or Google Cloud'),\n",
       " Document(metadata={'page': 2, 'type': 'text', '_id': '997e713d-c3fa-4f1b-8acf-a7386b323c18', '_collection_name': 'report'}, page_content='3/10/24, 8:32 PM\\nGemma: Google introduces new state-of-the-art open models\\nhttps://blog.google/technology/developers/gemma-open-models/\\n3/5\\ntheir sizes compared to other open models. And Gemma models are capable of running directly on a\\ndeveloper laptop or desktop computer. Notably, Gemma surpasses significantly larger models on key\\nbenchmarks while adhering to our rigorous standards for safe and responsible outputs. See the technical'),\n",
       " Document(metadata={'page': 1, 'type': 'text'}, page_content='collaboration, and guide responsible use of Gemma models.\\nGemma is available worldwide, starting today. Here are the key details to know:\\nWe’re releasing model weights in two sizes: Gemma 2B and Gemma 7B. Each size is released with pre-\\ntrained and instruction-tuned variants.\\nA new Responsible Generative AI Toolkit provides guidance and essential tools for creating safer AI\\napplications with Gemma.'),\n",
       " Document(metadata={'page': 3, 'type': 'text'}, page_content='Free credits for research and development\\nGemma is built for the open community of developers and researchers powering AI innovation. You can\\nstart working with Gemma today using free access in Kaggle, a free tier for Colab notebooks, and $300 in\\ncredits for first-time Google Cloud users. Researchers can also apply for Google Cloud credits of up to a\\ncollective $500,000 to accelerate their projects.')]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_retriever.invoke('quick start guide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "07143da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_multimodel(query,k=5):\n",
    "    #embed using clip\n",
    "    query_embedding=embed_text(query)\n",
    "    if not isinstance(query_embedding, list):\n",
    "            query_embedding = query_embedding.tolist()\n",
    "    # results=vectorstore.similarity_search_by_vector(\n",
    "    #     embedding=query_embedding,\n",
    "    #     k=k\n",
    "    # )\n",
    "    results=hybrid_retriever.get_relevant_documents(query)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bda6e4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 1, 'type': 'text', '_id': '056db1ac-9faa-466a-b37c-c4635e320756', '_collection_name': 'report'}, page_content='AI model widely available today. This enables Gemma 2B and 7B to achieve best-in-class performance for'),\n",
       " Document(metadata={'page': 4, 'type': 'text', '_id': 'd1903123-f576-4e8b-94ee-eaa9c1d549b7', '_collection_name': 'report'}, page_content='We’re excited to see what you create!\\nPOSTED IN:\\nDevelopers\\n \\nAI\\nMore Information\\nCollapse\\nGoogle adheres to rigorous data filtering practices to ensure fair evaluation. Our models exclude\\nbenchmark data from training sets, ensuring the integrity of benchmark comparisons.\\n1'),\n",
       " Document(metadata={'page': 2, 'type': 'text', '_id': '97719dc0-0857-4e7c-a90b-68c2b1b9466a', '_collection_name': 'report'}, page_content='researchers prioritize building safe and responsible AI applications. The toolkit includes:\\n1'),\n",
       " Document(metadata={'page': 1, 'type': 'text', '_id': '2ddd900a-7eff-4d33-b1be-77b711502d5d', '_collection_name': 'report'}, page_content='applications with Gemma.\\nWe’re providing toolchains for inference and supervised fine-tuning (SFT) across all major frameworks:\\nJAX, PyTorch, and TensorFlow through native Keras 3.0.\\nReady-to-use Colab and Kaggle notebooks, alongside integration with popular tools such as Hugging\\nFace, MaxText, NVIDIA NeMo and TensorRT-LLM, make it easy to get started with Gemma.\\nPre-trained and instruction-tuned Gemma models can run on your laptop, workstation, or Google Cloud'),\n",
       " Document(metadata={'page': 2, 'type': 'text', '_id': '823626db-dc87-4c2e-9271-f3f4d6fb465d', '_collection_name': 'report'}, page_content='3/10/24, 8:32 PM\\nGemma: Google introduces new state-of-the-art open models\\nhttps://blog.google/technology/developers/gemma-open-models/\\n3/5\\ntheir sizes compared to other open models. And Gemma models are capable of running directly on a\\ndeveloper laptop or desktop computer. Notably, Gemma surpasses significantly larger models on key\\nbenchmarks while adhering to our rigorous standards for safe and responsible outputs. See the technical')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"what's the company name\"\n",
    "retrieve_multimodel(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0a103a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodel_message(query,retrieved_docs):\n",
    "    content=[]\n",
    "    content.append({\n",
    "        'type':'text',\n",
    "        'text':f'Question:{query}\\n\\nContext:\\n'\n",
    "    })\n",
    "    text_docs=[doc for doc in retrieved_docs if doc.metadata.get('type')=='text']\n",
    "    image_docs=[doc for doc in retrieved_docs if doc.metadata.get('type')=='image']\n",
    "\n",
    "    if text_docs:\n",
    "        text_context='\\n\\n'.join([\n",
    "            f'[page {doc.metadata['page']}]: {doc.page_content}'\n",
    "            for doc in text_docs\n",
    "        ])\n",
    "        content.append({\n",
    "            'type':'text',\n",
    "            'text':f'text excerpts:\\n{text_context}\\n'\n",
    "        })\n",
    "\n",
    "    #add images\n",
    "    if image_docs:\n",
    "        image_id=doc.metadata.get('image_id')\n",
    "        if image_id and image_id in image_data_store:\n",
    "            content.append({\n",
    "                'type':'image',\n",
    "                'text':f'\\n[Image from page {doc.metadata['page']}]:\\n'\n",
    "            })\n",
    "            content.append({\n",
    "                'type':'image_url',\n",
    "                'image_url':{\n",
    "                    'url':f'data:image/png;base64,{image_data_store[image_id]}'\n",
    "                }\n",
    "            })  \n",
    "    content.append({\n",
    "        'type':'text',\n",
    "        'text':'\\nPlease answer the question based on the provided text and images'\n",
    "    })\n",
    "\n",
    "    return HumanMessage(content=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ba810374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "def multimodel_rag_pipeline(query):\n",
    "    # context_docs=retrieve_multimodel(query)\n",
    "    # message=create_multimodel_message(query,context_docs)\n",
    "    # response=llm.invoke([message])\n",
    "\n",
    "    # print(f'retrieved {len(context_docs)} documents')\n",
    "    # for doc in context_docs:\n",
    "    #     print(f'{doc.page_content[:50]}...\\n-------\\n')\n",
    "    \n",
    "    # return response\n",
    "    print('this is the query----------',query)\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=hybrid_retriever,\n",
    "    return_source_documents=True\n",
    "    )\n",
    "\n",
    "    result = qa.invoke({\"query\": query})\n",
    "    return result\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3ed08b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the query---------- which are some multiframework tools\n",
      "{'query': 'which are some multiframework tools', 'result': 'Here are a few of the multi‑framework tools that Google highlights for working with Gemma:\\n\\n| Category | Tools that span multiple frameworks |\\n|----------|--------------------------------------|\\n| **Core inference & fine‑tuning toolchains** | • **JAX**  <br>• **PyTorch**  <br>• **TensorFlow** (via native **Keras\\u202f3.0**) |\\n| **Integration libraries & ecosystems** | • **Hugging\\u202fFace** (model hub, 🤗\\u202fTransformers, 🤗\\u202fAccelerate)  <br>• **MaxText** (Google‑originated, works with JAX and TensorFlow)  <br>• **NVIDIA\\u202fNeMo** (supports PyTorch and TensorRT)  <br>• **TensorRT‑LLM** (optimizes models for NVIDIA GPUs, usable from PyTorch/TensorFlow pipelines) |\\n| **Convenient notebooks & platforms** | • **Google\\u202fColab** notebooks (pre‑configured for JAX, PyTorch, TF)  <br>• **Kaggle** notebooks (same multi‑framework support) |\\n| **Deployment back‑ends** | • **Vertex\\u202fAI** and **Google\\u202fKubernetes Engine (GKE)** – can serve models built in any of the three frameworks. |\\n| **Hardware‑agnostic optimizers** | • Optimizations that work across **NVIDIA GPUs** and **Google Cloud TPUs**, letting you run the same model regardless of whether it was trained in JAX, PyTorch, or TensorFlow. |\\n\\nThese tools let you develop, fine‑tune, and deploy Gemma models using whichever framework you prefer, while still being able to move the same model across JAX, PyTorch, and TensorFlow ecosystems.', 'source_documents': [Document(metadata={'page': 1, 'type': 'text', '_id': '8ade314c-b21f-4f7a-86c9-25f95c7c214c', '_collection_name': 'report'}, page_content='applications with Gemma.\\nWe’re providing toolchains for inference and supervised fine-tuning (SFT) across all major frameworks:\\nJAX, PyTorch, and TensorFlow through native Keras 3.0.\\nReady-to-use Colab and Kaggle notebooks, alongside integration with popular tools such as Hugging\\nFace, MaxText, NVIDIA NeMo and TensorRT-LLM, make it easy to get started with Gemma.\\nPre-trained and instruction-tuned Gemma models can run on your laptop, workstation, or Google Cloud'), Document(metadata={'page': 4, 'type': 'text', '_id': 'ec8d5a13-2881-49c0-b90b-321780ab7525', '_collection_name': 'report'}, page_content='We’re excited to see what you create!\\nPOSTED IN:\\nDevelopers\\n \\nAI\\nMore Information\\nCollapse\\nGoogle adheres to rigorous data filtering practices to ensure fair evaluation. Our models exclude\\nbenchmark data from training sets, ensuring the integrity of benchmark comparisons.\\n1'), Document(metadata={'page': 1, 'type': 'text', '_id': 'e1df1d04-efc1-48a9-818e-2bfe8480c155', '_collection_name': 'report'}, page_content='AI model widely available today. This enables Gemma 2B and 7B to achieve best-in-class performance for'), Document(metadata={'page': 2, 'type': 'text', '_id': 'f14e6203-6e1e-421f-ac74-546490480d30', '_collection_name': 'report'}, page_content='researchers prioritize building safe and responsible AI applications. The toolkit includes:\\n1'), Document(metadata={'page': 1, 'type': 'text', '_id': 'd9ca6353-d233-432f-a1f6-a786c3d6dda8', '_collection_name': 'report'}, page_content='with easy deployment on Vertex AI and Google Kubernetes Engine (GKE).\\nOptimization across multiple AI hardware platforms ensures industry-leading performance, including\\nNVIDIA GPUs and Google Cloud TPUs.\\nTerms of use permit responsible commercial usage and distribution for all organizations, regardless of\\nsize.\\nState-of-the-art performance at size\\nGemma models share technical and infrastructure components with Gemini, our largest and most capable'), Document(metadata={'page': 1, 'type': 'text'}, page_content='collaboration, and guide responsible use of Gemma models.\\nGemma is available worldwide, starting today. Here are the key details to know:\\nWe’re releasing model weights in two sizes: Gemma 2B and Gemma 7B. Each size is released with pre-\\ntrained and instruction-tuned variants.\\nA new Responsible Generative AI Toolkit provides guidance and essential tools for creating safer AI\\napplications with Gemma.'), Document(metadata={'page': 2, 'type': 'text'}, page_content='3/10/24, 8:32 PM\\nGemma: Google introduces new state-of-the-art open models\\nhttps://blog.google/technology/developers/gemma-open-models/\\n3/5\\ntheir sizes compared to other open models. And Gemma models are capable of running directly on a\\ndeveloper laptop or desktop computer. Notably, Gemma surpasses significantly larger models on key\\nbenchmarks while adhering to our rigorous standards for safe and responsible outputs. See the technical'), Document(metadata={'page': 2, 'type': 'text'}, page_content='from human feedback (RLHF) to align our instruction-tuned models with responsible behaviors. To\\nunderstand and reduce the risk profile for Gemma models, we conducted robust evaluations including\\nmanual red-teaming, automated adversarial testing, and assessments of model capabilities for dangerous\\nactivities. These evaluations are outlined in our Model Card.\\nWe’re also releasing a new Responsible Generative AI Toolkit together with Gemma to help developers and')]}\n"
     ]
    }
   ],
   "source": [
    "response=multimodel_rag_pipeline('which are some multiframework tools')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "d13d9c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import OpenWeatherMapAPIWrapper\n",
    "from langchain_community.tools import OpenWeatherMapQueryRun\n",
    "weather_wrapper = OpenWeatherMapAPIWrapper()\n",
    "weather=OpenWeatherMapQueryRun(api_wrapper=weather_wrapper)\n",
    "\n",
    "def rag(state:State):\n",
    "    '''\n",
    "    Args:\n",
    "        query: user input\n",
    "    To return answers any user questions\n",
    "    '''\n",
    "    response=multimodel_rag_pipeline(state['messages'][-1].content)\n",
    "    print(f'this is the summary------{response}')\n",
    "    return {'messages':[AIMessage(content=response['result'])]}\n",
    "\n",
    "tools=[weather]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d1b10ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools=llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "cc1d41f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory=MemorySaver()\n",
    "\n",
    "def chatbot(state:State):\n",
    "    response = llm_with_tools.invoke(state['messages'])\n",
    "    print(f'response from chatbot -- {response}')\n",
    "    print(\"\\n[Chatbot Node Called] Messages so far:\")\n",
    "    for msg in state['messages']:\n",
    "        print(msg)\n",
    "    if response.content:\n",
    "        print('no tool call -------')\n",
    "        return state\n",
    "    return {\"messages\":[response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "cec9905d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "graph=StateGraph(State)\n",
    "\n",
    "## node\n",
    "graph.add_node(\"Chatbot\",chatbot)\n",
    "graph.add_node(\"tools\", ToolNode(tools))\n",
    "graph.add_node(\"rag\",rag)\n",
    "\n",
    "## Edges\n",
    "graph.add_edge(START,\"Chatbot\")\n",
    "graph.add_conditional_edges(\n",
    "    \"Chatbot\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition,\n",
    "    {\n",
    "        'tools': \"tools\",         # if condition returns True -> tools\n",
    "        '__end__': \"rag\"   # if condition returns False -> AnotherNode\n",
    "    }\n",
    ")\n",
    "graph.add_edge('tools',END)\n",
    "graph.add_edge('rag',END)\n",
    "graph_builder=graph.compile(checkpointer=memory)\n",
    "\n",
    "\n",
    "## Display\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph_builder.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "4feea45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response from chatbot -- content='I’m happy to help, but I need a bit more information. Could you let me know which report you’d like summarized (e.g., its title, subject, or a copy of the text)? Once I have the report details, I can provide a concise summary for you.' additional_kwargs={'reasoning_content': 'The user says \"summary of the report\". We need context: There\\'s no prior conversation. Likely they want a summary of some report, but we don\\'t have the report content. Maybe they refer to some report we have? Could be they want a summary of a report we have in the system? There\\'s no attached file. Might need to ask for clarification. But maybe they expect a generic answer? Could ask which report. Probably need to ask for more details.'} response_metadata={'token_usage': {'completion_tokens': 160, 'prompt_tokens': 151, 'total_tokens': 311, 'completion_time': 0.333900159, 'prompt_time': 0.007727242, 'queue_time': 0.042674268, 'total_time': 0.341627401}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_213abb2467', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--48b4e2e3-792c-4ed4-b7cb-962a75b065ed-0' usage_metadata={'input_tokens': 151, 'output_tokens': 160, 'total_tokens': 311}\n",
      "\n",
      "[Chatbot Node Called] Messages so far:\n",
      "content='summary of the report' additional_kwargs={} response_metadata={} id='71a697af-1817-4f1c-bac8-e60db8aecf0f'\n",
      "no tool call -------\n",
      "this is the query---------- summary of the report\n",
      "this is the summary------{'query': 'summary of the report', 'result': '**Summary of the Gemma Model Report**\\n\\n| Aspect | Key Points |\\n|--------|------------|\\n| **Purpose & Scope** | The report documents the development, evaluation, and responsible‑use considerations for the Gemma family of open‑source language models (2\\u202fB and 7\\u202fB parameters). |\\n| **Model Architecture** | Gemma builds on the same research and technology that powers Google’s Gemini models, delivering state‑of‑the‑art performance in a lightweight package. |\\n| **Training Data** | • Rigorous data‑filtering pipelines were applied to remove personal information, other sensitive content, and benchmark data that could bias evaluation.<br>• The final training corpus is a curated mix of publicly available text, filtered for quality and safety. |\\n| **Performance** | • Benchmarks show best‑in‑class results for models of this size across a range of language‑understanding and generation tasks.<br>• Detailed performance tables (accuracy, perplexity, etc.) are provided in the full report. |\\n| **Safety & Responsible AI** | • **Automated filtering** of training data to strip personal and sensitive information.<br>• **Fine‑tuning with RLHF** (Reinforcement Learning from Human Feedback) to align the models with helpful and harmless behavior.<br>• **Robust evaluation** that includes:<br>\\u2003- Manual red‑team testing.<br>\\u2003- Automated adversarial attacks.<br>\\u2003- Assessment of capabilities for dangerous activities. |\\n| **Risk Assessment** | The report outlines a risk profile covering misuse potential, bias, and privacy concerns, and describes mitigation strategies (e.g., usage guidelines, monitoring tools). |\\n| **Responsible Generative AI Toolkit** | A companion toolkit is released alongside the model weights, offering: <br>• Guidance for developers on safe deployment.<br>• Utilities for content filtering, usage‑policy enforcement, and monitoring.<br>• Best‑practice documentation for building responsible AI applications. |\\n| **Transparency** | All model weights, training scripts, and the full Model Card are publicly released, enabling independent verification and further research. |\\n| **Future Work** | Planned improvements include expanding safety datasets, refining RLHF pipelines, and adding more extensive multilingual benchmarks. |\\n\\n**Takeaway:**  \\nThe Gemma report demonstrates that Google DeepMind has created lightweight, high‑performing language models while prioritizing safety and responsible AI. By combining rigorous data curation, extensive fine‑tuning with human feedback, and thorough risk‑assessment testing, the team provides both powerful models and a suite of tools to help developers deploy them responsibly. All materials (weights, code, Model Card, and toolkit) are openly available for the research community.', 'source_documents': [Document(metadata={'page': 4, 'type': 'text', '_id': 'ec8d5a13-2881-49c0-b90b-321780ab7525', '_collection_name': 'report'}, page_content='We’re excited to see what you create!\\nPOSTED IN:\\nDevelopers\\n \\nAI\\nMore Information\\nCollapse\\nGoogle adheres to rigorous data filtering practices to ensure fair evaluation. Our models exclude\\nbenchmark data from training sets, ensuring the integrity of benchmark comparisons.\\n1'), Document(metadata={'page': 2, 'type': 'text', '_id': '1fefb124-1d48-409c-8365-5224fab4927c', '_collection_name': 'report'}, page_content='report for details on performance, dataset composition, and modeling methodologies.\\nResponsible by design\\nGemma is designed with our AI Principles at the forefront. As part of making Gemma pre-trained models\\nsafe and reliable, we used automated techniques to filter out certain personal information and other\\nsensitive data from training sets. Additionally, we used extensive fine-tuning and reinforcement learning'), Document(metadata={'page': 1, 'type': 'text', '_id': 'e1df1d04-efc1-48a9-818e-2bfe8480c155', '_collection_name': 'report'}, page_content='AI model widely available today. This enables Gemma 2B and 7B to achieve best-in-class performance for'), Document(metadata={'page': 2, 'type': 'text', '_id': 'f14e6203-6e1e-421f-ac74-546490480d30', '_collection_name': 'report'}, page_content='researchers prioritize building safe and responsible AI applications. The toolkit includes:\\n1'), Document(metadata={'page': 2, 'type': 'text', '_id': 'acfc5189-6f21-4689-b2d9-fddc26a62aa6', '_collection_name': 'report'}, page_content='from human feedback (RLHF) to align our instruction-tuned models with responsible behaviors. To\\nunderstand and reduce the risk profile for Gemma models, we conducted robust evaluations including\\nmanual red-teaming, automated adversarial testing, and assessments of model capabilities for dangerous\\nactivities. These evaluations are outlined in our Model Card.\\nWe’re also releasing a new Responsible Generative AI Toolkit together with Gemma to help developers and'), Document(metadata={'page': 1, 'type': 'text'}, page_content='researchers in building AI responsibly.\\nGemma open models\\nGemma is a family of lightweight, state-of-the-art open models built from the same research and\\ntechnology used to create the Gemini models. Developed by Google DeepMind and other teams across\\nGoogle, Gemma is inspired by Gemini, and the name reflects the Latin gemma, meaning “precious stone.”\\nAccompanying our model weights, we’re also releasing tools to support developer innovation, foster')]}\n"
     ]
    }
   ],
   "source": [
    "## Invocation\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "out=graph_builder.invoke({'messages':\"summary of the report\"},config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "b47ef6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Summary of the Gemma Model Report**\n",
      "\n",
      "| Aspect | Key Points |\n",
      "|--------|------------|\n",
      "| **Purpose & Scope** | The report documents the development, evaluation, and responsible‑use considerations for the Gemma family of open‑source language models (2 B and 7 B parameters). |\n",
      "| **Model Architecture** | Gemma builds on the same research and technology that powers Google’s Gemini models, delivering state‑of‑the‑art performance in a lightweight package. |\n",
      "| **Training Data** | • Rigorous data‑filtering pipelines were applied to remove personal information, other sensitive content, and benchmark data that could bias evaluation.<br>• The final training corpus is a curated mix of publicly available text, filtered for quality and safety. |\n",
      "| **Performance** | • Benchmarks show best‑in‑class results for models of this size across a range of language‑understanding and generation tasks.<br>• Detailed performance tables (accuracy, perplexity, etc.) are provided in the full report. |\n",
      "| **Safety & Responsible AI** | • **Automated filtering** of training data to strip personal and sensitive information.<br>• **Fine‑tuning with RLHF** (Reinforcement Learning from Human Feedback) to align the models with helpful and harmless behavior.<br>• **Robust evaluation** that includes:<br> - Manual red‑team testing.<br> - Automated adversarial attacks.<br> - Assessment of capabilities for dangerous activities. |\n",
      "| **Risk Assessment** | The report outlines a risk profile covering misuse potential, bias, and privacy concerns, and describes mitigation strategies (e.g., usage guidelines, monitoring tools). |\n",
      "| **Responsible Generative AI Toolkit** | A companion toolkit is released alongside the model weights, offering: <br>• Guidance for developers on safe deployment.<br>• Utilities for content filtering, usage‑policy enforcement, and monitoring.<br>• Best‑practice documentation for building responsible AI applications. |\n",
      "| **Transparency** | All model weights, training scripts, and the full Model Card are publicly released, enabling independent verification and further research. |\n",
      "| **Future Work** | Planned improvements include expanding safety datasets, refining RLHF pipelines, and adding more extensive multilingual benchmarks. |\n",
      "\n",
      "**Takeaway:**  \n",
      "The Gemma report demonstrates that Google DeepMind has created lightweight, high‑performing language models while prioritizing safety and responsible AI. By combining rigorous data curation, extensive fine‑tuning with human feedback, and thorough risk‑assessment testing, the team provides both powerful models and a suite of tools to help developers deploy them responsibly. All materials (weights, code, Model Card, and toolkit) are openly available for the research community.\n"
     ]
    }
   ],
   "source": [
    "print(out['messages'][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (assigment)",
   "language": "python",
   "name": "uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
